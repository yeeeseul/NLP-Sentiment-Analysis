{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "yyoo0548_COMP5046_Ass1.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "GXgFpxIgl-_G",
        "-2feNpG-LZx2",
        "4zFo6YppL6w3"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGHoy6KpQDfZ",
        "colab_type": "text"
      },
      "source": [
        "# COMP5046 Assignment 1\n",
        "*Make sure you change the file name with your unikey.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34DVNKgqQY21",
        "colab_type": "text"
      },
      "source": [
        "# 1 - Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7cWUxAQrGlq6",
        "colab_type": "text"
      },
      "source": [
        "## 1.1. Download Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RUa2IBps87rW",
        "colab_type": "code",
        "outputId": "a9e373c2-a39c-48fa-fc52-069561a5e2b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Code to download file into Colaboratory:\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "id = '1vF3FqgBC1Y-RPefeVmY8zetdZG1jmHzT'\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('imdb_train.csv')\n",
        "\n",
        "id = '1XhaV8YMuQeSwozQww8PeyiWMJfia13G6'\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('imdb_test.csv')\n",
        "\n",
        "import pandas as pd\n",
        "df_train = pd.read_csv(\"imdb_train.csv\")\n",
        "df_test = pd.read_csv(\"imdb_test.csv\")\n",
        "\n",
        "reviews_train = df_train['review'].tolist()\n",
        "sentiments_train = df_train['sentiment'].tolist()\n",
        "reviews_test = df_test['review'].tolist()\n",
        "sentiments_test = df_test['sentiment'].tolist()\n",
        "\n",
        "print(\"Training set number:\",len(reviews_train))\n",
        "print(\"Testing set number:\",len(reviews_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training set number: 25000\n",
            "Testing set number: 25000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9gBSgBCQh24",
        "colab_type": "text"
      },
      "source": [
        "## 1.2. Preprocess data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8RdKI8E2KRwe",
        "colab_type": "text"
      },
      "source": [
        "For preprocess, I converted all the character to lowercase and removed punctuation characters, first of all. After this, I did stemming, tokenisation, removing stopwords, and lemmatisation as well.\n",
        "\n",
        "*   Converting to lowercase : To minimise the number of word dictionary and to avoid creating several dictionary items with exactly same meaning.\n",
        "*   Removing punctuation/special characters : To avoid additional creation of dictionary items. Moreover, I found that many sentences had 'br /' so decided to remove them all in order.\n",
        "*   Stemming : To reduce the number of word by integreting the words which have same stems.\n",
        "*   Tokenisation : To approach sentences in terms of words.\n",
        "*   Removing stopwords : Stopwords such as  “the”, “is”, “in”, “for”, “where”, “when”, “to”, and “at” are not much meaningful in the context so they are removed.\n",
        "*   Lemmatisaion : To make word dictionary simpler, all the words transformed to a single item or dictionary form, in the other words.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vQZwL_Nvtn1I"
      },
      "source": [
        "**Converting to lowercase and removing punctuation/special characters**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EUqSi1RP8gup",
        "colab": {}
      },
      "source": [
        "import re\n",
        "puncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£', \n",
        " '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',  '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', \n",
        " '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', \n",
        " '▒', '：', '¼', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲', 'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', \n",
        " '∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√', \n",
        " 'å', 'ô', 'ä', 'á', 'í', 'ý', 'ō', 'ü', 'ì', 'ã', 'ê', 'ç', 'ß', 'ó', 'ø', 'ú', 'ñ', 'æ', 'ö', 'ò', 'û', 'ë', 'ù', 'î', 'º', 'õ', 'ð',\n",
        " 'ª', 'á', 'ä', 'å', 'æ', 'ç', 'è', 'é', 'ê', 'ï', 'ð', 'ó', 'õ', 'ö', 'ü', 'þ', 'ğ', 'ı', 'ż', 'א', 'ג', 'ו', 'י', 'כ', 'ל', 'מ', 'ן', 'ר']\n",
        "\n",
        "brs = ['<br /><br />', '<br />']\n",
        "\n",
        "reviews_train = [i.lower() for i in reviews_train]\n",
        "reviews_test = [i.lower() for i in reviews_test]\n",
        "\n",
        "def preprocess(x):\n",
        "    x = str(x)\n",
        "    for br in brs:\n",
        "        if br in x:\n",
        "            x = x.replace(br, ' ')\n",
        "    for punct in puncts:\n",
        "        if punct in x:\n",
        "            x = x.replace(punct, '')\n",
        "    x = re.sub(r'[^\\w\\s]','', x)\n",
        "    x = re.sub('[0-9]+', '', x)\n",
        "    return x\n",
        "\n",
        "#review_train_test = [preprocess(s) for s in review_train_test]\n",
        "reviews_train = [preprocess(s) for s in reviews_train]\n",
        "reviews_test = [preprocess(s) for s in reviews_test]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CvbP6uS1siDa",
        "colab_type": "text"
      },
      "source": [
        "**Tokenisation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "cf0f8665-7294-4485-c778-6f91d3bd178e",
        "id": "fMQ1YYxA8gu9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "#review_train_test_tk = [word_tokenize(i) for i in review_train_test]\n",
        "\n",
        "reviews_train_tk = [word_tokenize(i) for i in reviews_train]\n",
        "reviews_test_tk = [word_tokenize(i) for i in reviews_test]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RCxzb20W13f_",
        "colab_type": "text"
      },
      "source": [
        "**Stemming**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xUCpB3yR8gu4",
        "colab": {}
      },
      "source": [
        "from nltk.stem.porter import *\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "reviews_train_st = []\n",
        "for tokens in reviews_train_tk:\n",
        "    stem_sentence = [stemmer.stem(w) for w in tokens ]\n",
        "    reviews_train_st.append(stem_sentence)\n",
        "\n",
        "reviews_test_st = []\n",
        "for tokens in reviews_test_tk:\n",
        "    stem_sentence = [stemmer.stem(w) for w in tokens ]\n",
        "    reviews_test_st.append(stem_sentence)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "mcrxNHqa8gu3"
      },
      "source": [
        "**Lemmatisation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6BTXGVwjLuQN",
        "outputId": "8c83f811-804d-4071-daa0-484ce2adb1a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "nltk.download('wordnet')\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "reviews_train_le = []\n",
        "for tokens in reviews_train_st:\n",
        "    lemma_sentence = [lemmatizer.lemmatize(w) for w in tokens ]\n",
        "    reviews_train_le.append(lemma_sentence)\n",
        "\n",
        "reviews_test_le = []\n",
        "for tokens in reviews_test_st:\n",
        "    lemma_sentence = [lemmatizer.lemmatize(w) for w in tokens ]\n",
        "    reviews_test_le.append(lemma_sentence)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1rinkhDssqw5"
      },
      "source": [
        "**Remove stopwords**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "9310f806-a50a-45a5-d180-ffe7e5c2c565",
        "id": "yAuiO2YM8gvA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords as sw\n",
        "stop_words = sw.words()\n",
        "\n",
        "reviews_train_fin=[]\n",
        "for tokens in reviews_train_le:\n",
        "    filtered_sentence = [w for w in tokens if not w in stop_words]\n",
        "    reviews_train_fin.append(filtered_sentence)\n",
        "\n",
        "reviews_test_fin=[]\n",
        "for tokens in reviews_test_le:\n",
        "    filtered_sentence = [w for w in tokens if not w in stop_words]\n",
        "    reviews_test_fin.append(filtered_sentence)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LIu_lkJwQ55g",
        "colab_type": "text"
      },
      "source": [
        "# 2 - Model Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "daDvAftceIvr",
        "colab_type": "text"
      },
      "source": [
        "## 2.1. Word Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbzm-NWBTmM-",
        "colab_type": "text"
      },
      "source": [
        "* For word embedding, skip-gram model has been implemented. This project aims sentiment analysis so I thought that skip-gram is more appropriate since it predicts the context given a word. Sentiment tends to be determined using context so skip-gram model might be more meaningful than cbow."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXgFpxIgl-_G",
        "colab_type": "text"
      },
      "source": [
        "### 2.1.1. Data Preprocessing for Word Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QoKNMMTraAOg",
        "colab_type": "text"
      },
      "source": [
        "* For detail explanation of preprocessing skills for this part, please see 1.2 Preprocess data. I thought that applying the same preprocessing skills used earlier for general data preprocessing should be fine here because I believe it  basically might be inplemented in order to make data cleaner and smaller."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3LByzHLiNinu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "word_sequence = []\n",
        "word_list = []\n",
        "\n",
        "reviews_train_test_fin = reviews_train_fin + reviews_test_fin\n",
        "\n",
        "word_sequence = [val for sublist in reviews_train_test_fin for val in sublist]\n",
        "word_list = [val for sublist in reviews_train_test_fin for val in sublist]\n",
        "\n",
        "# make a list which contains unique words from train set and test set combined\n",
        "word_list = list(set(word_list))\n",
        "word_list.sort()\n",
        "\n",
        "# make dictionary so that we can be reference each index of unique word\n",
        "word_dict = {w: i for i, w in enumerate(word_list)}\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5U1Uv6Tu8Rze",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = list([item for sublist in reviews_train_test_fin for item in sublist])\n",
        "np.savetxt('train_and_test_combined_2.txt', x, fmt='%s')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qhAgWf_AmbZ8",
        "colab_type": "text"
      },
      "source": [
        "### 2.1.2. Build Word Embeddings Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZf4SOukucEO",
        "colab_type": "text"
      },
      "source": [
        "* Window size has set by 1. I desirely wanted to make the size even bigger, but in terms of the running time, I just justified it as 1. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K0GOT9fxAinv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "window_size = 1\n",
        "skip_grams = []\n",
        "\n",
        "for i in range (len(word_sequence)-window_size):\n",
        "    # (context, target) : ([target index - window_size ~ target index + window_size], target)\n",
        "    target = word_dict[word_sequence[i]]\n",
        "    context = [word_dict[word_sequence[i - window_size]], word_dict[word_sequence[i + window_size]]]\n",
        "\n",
        "    # make a skip_grams list which contains context words and target words\n",
        "    for w in context:\n",
        "        skip_grams.append([target, w])\n",
        "      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TVPuwWgvNjOU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# declare the size of vocabulary same as the size(length) of word_list \n",
        "# which contains unique words in my dataset to train the model\n",
        "voc_size = len(word_list)\n",
        "\n",
        "def prepare_batch(data, size):\n",
        "    # randomly pick up the inputs and targets for the model implementation\n",
        "    random_inputs = []\n",
        "    random_labels = []\n",
        "    random_index = np.random.choice(range(len(data)), size, replace=False)\n",
        "\n",
        "    for i in random_index:\n",
        "        input_temp = [0]*voc_size\n",
        "        # one hot encoding\n",
        "        input_temp[data[i][0]] = 1\n",
        "        random_inputs.append(input_temp)  # target\n",
        "        random_labels.append(data[i][1])  # context word\n",
        "    \n",
        "    # return input and target array\n",
        "    return np.array(random_inputs), np.array(random_labels)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2szlNvvEff8-",
        "colab_type": "text"
      },
      "source": [
        "* Unfortunately, my end keeps crashing during training the model with larger number of epochs, so I just decided to stick with a relatively small number of epoch. </br>\n",
        "* Too large learning rate can make gradient descent increase but too small learning rate can occur a longer training time training and may keep represent a high training error. So, here, I just used 0.01, which is known as a default value. However, the model is expected to be improved with a better learning rate, I assume."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QYQ39X81xlsj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learning_rate = 0.01\n",
        "batch_size = 500\n",
        "embedding_size = 128\n",
        "no_of_epochs = 500"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vSA5INpJHELg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "############ SkipGram Model\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "class WordEmb(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(WordEmb, self).__init__()\n",
        "        self.linear1 = nn.Linear(voc_size, embedding_size, bias=False)\n",
        "        self.linear2 = nn.Linear(embedding_size, voc_size, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        hidden = self.linear1(x)\n",
        "        out = self.linear2(hidden)\n",
        "        return out\n",
        "\n",
        "skip_gram_model = WordEmb()\n",
        "criterion = nn.CrossEntropyLoss() \n",
        "# Implements stochastic gradient descent\n",
        "optimiser = optim.SGD(skip_gram_model.parameters(), lr=learning_rate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qFH-PPClqxzF",
        "colab_type": "text"
      },
      "source": [
        "* CrossEntropyLoss used to detect the loss between input and target(class). </br>\n",
        "MSELoss tends to more focus on the sample where is incorrectly classified, so I used CrossEntrophyLoss which is more appropriate for classifying problem\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LNys5HOdISK-",
        "colab_type": "text"
      },
      "source": [
        "### 2.1.3. Train Word Embeddings Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z1dHG_yt-ohg",
        "colab_type": "code",
        "outputId": "b58cbd0d-6863-4030-f17e-674fa16d11fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "############ SkipGram Model Training\n",
        "\n",
        "# Print loss every 50 epoch\n",
        "display_interval = 50\n",
        "\n",
        "for epoch in range(no_of_epochs):\n",
        "    \n",
        "    # train the model\n",
        "    skip_gram_model.train()\n",
        "\n",
        "    inputs,labels = prepare_batch(skip_grams, batch_size)\n",
        "\n",
        "    # making torch for inputs and labels(target)\n",
        "    inputs_torch = torch.from_numpy(inputs).float()\n",
        "    labels_torch = torch.from_numpy(labels)\n",
        "    # 1. zero grad\n",
        "    optimiser.zero_grad()\n",
        "    # 2. forword propagation\n",
        "    outputs = skip_gram_model(inputs_torch)\n",
        "    # 3. calculate loss\n",
        "    loss = criterion(outputs, labels_torch)\n",
        "    # 4. back propagation\n",
        "    loss.backward()\n",
        "    optimiser.step()\n",
        "    \n",
        "    if epoch % display_interval == display_interval-1 : \n",
        "        print('Epoch: %d, loss: %.4f' %(epoch + 1, loss))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 100, loss: 11.7213\n",
            "Epoch: 150, loss: 11.7213\n",
            "Epoch: 200, loss: 11.7214\n",
            "Epoch: 250, loss: 11.7213\n",
            "Epoch: 300, loss: 11.7213\n",
            "Epoch: 350, loss: 11.7213\n",
            "Epoch: 400, loss: 11.7213\n",
            "Epoch: 450, loss: 11.7213\n",
            "Epoch: 500, loss: 11.7212\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g9TqstzzvAHA",
        "colab_type": "text"
      },
      "source": [
        "* As you can see above, loss is pretty high and never decreases somehow. When I train my code with smaller subset of given dataset, it shows loss decreasing. Hence, this might have happened because I miss-selected hyperparameters for the model that used this huge dataset and I assume it would be perfoming better with appropriate hyperparameters. I was just frustrated of too long runtime and crash of colab frequently on my end everytime I trained this model. (I put my all effort on this several days in a row, but I ended up submitting this kind of underperfomed model. I would like to hear some advice if you(who is reading this) are available! That would be appreciated. )"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vNVZWIpU36d1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# get the weight of skip_gram model's linear layer to apply for character embedding\n",
        "weight1 = skip_gram_model.linear1.weight\n",
        "trained_word_embeddings = weight1.detach().T.numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xbZ4CkQd-Te0",
        "colab_type": "code",
        "outputId": "9ed965a7-89bf-40e5-b202-fb49a1f8135d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        " trained_word_embeddings"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-1.6977769e-04,  1.1572820e-03, -1.3851508e-03, ...,\n",
              "         8.6256239e-04, -1.4787194e-03,  1.3523576e-03],\n",
              "       [ 2.2522414e-03,  1.7776175e-03,  2.0616867e-03, ...,\n",
              "        -1.2172499e-03, -1.9188643e-03,  8.3156698e-04],\n",
              "       [ 1.0878297e-03, -9.4488519e-04,  3.2123527e-04, ...,\n",
              "         3.7756399e-04, -1.5337992e-03,  6.8635051e-04],\n",
              "       ...,\n",
              "       [ 2.7849500e-03, -2.8034882e-04,  1.9798458e-03, ...,\n",
              "         1.8696429e-03,  6.3267373e-04, -2.6434951e-03],\n",
              "       [-2.2919350e-03,  5.5287965e-05,  4.4457498e-04, ...,\n",
              "        -4.1505904e-04, -2.4158328e-03, -2.2661367e-03],\n",
              "       [ 2.0977396e-03, -1.7159763e-03,  2.4131716e-03, ...,\n",
              "        -1.2327908e-03, -2.9526860e-04,  2.0719527e-03]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MURhXrF57W2_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# save the weight to be easily used later on even if the session is down\n",
        "np.savetxt('word_embedding_weight.txt', trained_word_embeddings)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMCv3YI1IfUo",
        "colab_type": "text"
      },
      "source": [
        "### 2.1.4. Save Word Embeddings Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VWiMyigydDi5",
        "colab_type": "code",
        "outputId": "2e7871cb-3dd4-4720-97c8-5cec009edd19",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "#Save model into google drive\n",
        "torch.save(skip_gram_model, 'word_model_2.pt')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:360: UserWarning: Couldn't retrieve source code for container of type WordEmb. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yn16xrDrIs8B",
        "colab_type": "text"
      },
      "source": [
        "### 2.1.5. Load Word Embeddings Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J2NEfeF8WxB1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "downloaded = drive.CreateFile({'id':\"1D1MqnSYe9Ug8BHkoTIMrzj9fc7dXSL46\"})\n",
        "downloaded.GetContentFile('word_model_2.pt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8dBE_esjXhPM",
        "colab_type": "code",
        "outputId": "3b0018e0-d685-4964-eb59-9d29e7c487bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "word_model = torch.load('word_model_2.pt')\n",
        "word_model.eval()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "WordEmb(\n",
              "  (linear1): Linear(in_features=123166, out_features=128, bias=False)\n",
              "  (linear2): Linear(in_features=128, out_features=123166, bias=False)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "T0ap96aeGlIk"
      },
      "source": [
        "## 2.2. Character Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1XvNZdgJVzv",
        "colab_type": "text"
      },
      "source": [
        "*  For character embedding, I used Bi-LSTM model. Bi-LSTM is composed of both forward and backward LSTM sequences. The combined output embeddings from the forward and backward LSTM simultaneously encode information from both past (backwards) and future (forward) states. Each word representation coming out of this layer now includes contextual information about the surrounding phrases of the word. (Meraldo Antonio, https://towardsdatascience.com/the-definitive-guide-to-bidaf-part-2-word-embedding-character-embedding-and-contextual-c151fc4f05bb)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "d16v3oKaGlI0"
      },
      "source": [
        "### 2.2.1. Data Preprocessing for Character Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xIgr4ILsw2s8"
      },
      "source": [
        "* For detail explanation of preprocessing skills for this part, please see 1.2 Preprocess data.\n",
        "* To make every vectors have a same shape, I did padding for every words I had. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2MFbwQGl6uc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# To train the character model easily, \n",
        "# I loaded the weights of the word model that I saved earlier into txt format\n",
        "\n",
        "downloaded = drive.CreateFile({'id':\"1DrKQqLNO8A8r2kj359auPD1Shg8nKsbL\"})\n",
        "downloaded.GetContentFile('word_embedding_weight.txt')\n",
        "\n",
        "trained_embeddings = np.loadtxt('word_embedding_weight.txt', dtype=float)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3uxmFePEvDao",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This will be used in the next step\n",
        "def padding_word(corpus, word_length): # word padding\n",
        "    output = []\n",
        "    if len(corpus) > word_length:\n",
        "      output.append(corpus[:word_length])\n",
        "    else: # when the word is shorter than the maximum word length\n",
        "      for j in range(word_length - len(corpus)):\n",
        "        # padding 0 as much as a current word needs to achieve the maximum length\n",
        "        corpus.append(0)\n",
        "      output.append(corpus)\n",
        "    return corpus"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6T5lzI3kI8kp",
        "colab_type": "text"
      },
      "source": [
        "* To map each character in the dataset, I made a dictionary like below "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kWKym7136ut5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# Character Dictionary\n",
        "char_arr = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', \n",
        "            'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z',\n",
        "            '1', '2', '3', '4', '5', '6', '7', '8', '9', '0']\n",
        "\n",
        "# Making a dictionary matching with char_arr list\n",
        "num_dic = {n: i for i, n in enumerate(char_arr)}\n",
        "dic_len = len(num_dic)\n",
        "\n",
        "#pp = reviews_train_fin + reviews_test_fin\n",
        "word_in_list = []\n",
        "word_in_list = list([item for sublist in reviews_train_test_fin for item in sublist])\n",
        "word_in_list = list(set(word_in_list))\n",
        "word_in_list.sort()\n",
        "\n",
        "word_length = len(max((word for L in reviews_train_test_fin for word in L), key=len))\n",
        "\n",
        "#reviews_train_to_int = [] \n",
        "\n",
        "# Making a batch with padding\n",
        "input_batch_all = []\n",
        "\n",
        "for seq in word_in_list :\n",
        "  input_data = [num_dic[n] for n in seq]\n",
        "  input_data = padding_word(input_data, word_length)\n",
        "  input_batch_all.append(np.eye(dic_len)[input_data]) \n",
        "#reviews_train_to_int.append(input_data)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lwGe61W5gv6K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trained_embeddings"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zgiOPcsTGlI6"
      },
      "source": [
        "### 2.2.2. Build Character Embeddings Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bhQR1Ds2wJ_H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Generate a mini batch \n",
        "\n",
        "def gen_batch(input_embedding, batch_size) :\n",
        "  input_array = np.array(input_embedding)\n",
        "\n",
        "  # setting index randomly as a size of batch size \n",
        "  idx = np.random.randint(len(input_array), size=batch_size)\n",
        "\n",
        "  # get the final batch \n",
        "  batch_fin = input_array[idx,:,:]\n",
        "  return batch_fin, idx\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4O_yVK9XynuV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Setting hyperparameters\n",
        "\n",
        "n_hidden = 64\n",
        "n_input = dic_len"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K3851esDyrXU",
        "colab_type": "text"
      },
      "source": [
        "* I set the embedding size as 128 for word embedding model like above, the number of hidden layers had to be 64. After foward and back propogation, this number will be doubled so it matches with 128.\n",
        "* As a character based embedding, the number of input should be same with the number of components in a dictionary that I made above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e6w5S4p4M9ma",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Model building\n",
        "\n",
        "import torch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "class CharEmb(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CharEmb, self).__init__()\n",
        "        # Apply dropout to prevent overfitting\n",
        "        self.lstm = nn.LSTM(n_input, n_hidden, batch_first = True, bidirectional=True, dropout=0.2)\n",
        "\n",
        "    def forward(self, sentence):        \n",
        "        # h_n of shape (num_layers * num_directions, batch, hidden_size): \n",
        "        # tensor containing the hidden state for t = seq_len.\n",
        "        lstm_out, (h_n,c_n) = self.lstm(sentence)\n",
        "        # concat the last hidden state from two direction\n",
        "        hidden_out = torch.cat((h_n[0,:,:],h_n[1,:,:]),1)\n",
        "        return hidden_out\n",
        "        \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "46W0zFfWGlI_"
      },
      "source": [
        "### 2.2.3. Train Character Embeddings Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pHI8lyLkyx2p",
        "colab_type": "text"
      },
      "source": [
        "* MSELoss is appropriate when the target is relatively normally distributed. I was not able to look through if my targets are normally distributed, but I used MSELoss since it is commonly used for regression problem like I have.\n",
        "* For learning rate, once I change it from 0.001 to 0.3 and captured the difference(which is good). I realised that 0.001 was too small for this model. I also came up with 0.5, which is relatively higher, and it worked worse than where learning rate was 0.3, so I decided to settle down at 0.3.\n",
        "* Batch size set to 500 and number of epochs set to 1000, I believe they are not enough to train the model beautifully, but for shorter running time to finish the assignment.\n",
        "* Adam optimiser is considered one of the best optimisers nowadays (and I always like the one called 'best'!) and this controls the weight adaptation better than simple SGD during first iterations/epocs (Danny Rosen (https://stats.stackexchange.com/users/248371/danny-rosen), What is the reason that the Adam Optimizer is considered robust to the value of its hyper parameters?)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hk9P4T1VM92n",
        "colab_type": "code",
        "outputId": "4f1d77d0-4183-4086-a375-20b3b9944171",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Model training\n",
        "\n",
        "# Move the model to GPU\n",
        "character_embedding = CharEmb().to(device)\n",
        "\n",
        "# Setting hyperparameters\n",
        "\n",
        "no_of_epochs = 1000\n",
        "learning_rate = 0.3\n",
        "batch_size = 500\n",
        "\n",
        "display_interval = 10\n",
        "\n",
        "# Loss function and optimizer\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(character_embedding.parameters(), lr=learning_rate)\n",
        "\n",
        "for epoch in range(no_of_epochs):  \n",
        "    # Declare input and target\n",
        "    input_batch, idx = gen_batch(input_batch_all, batch_size)\n",
        "    # Matching input and target batchs using idx \n",
        "    target_batch = trained_embeddings[idx,:]\n",
        "\n",
        "    # Convert input into tensors and move them to GPU by uting tensor.to(device)\n",
        "    input_batch_torch = torch.from_numpy(np.array(input_batch)).float().to(device)\n",
        "    target_batch_torch = torch.from_numpy(target_batch).float().to(device)\n",
        "    \n",
        "    # Set the flag to training\n",
        "    character_embedding.train()\n",
        "  \n",
        "\n",
        "    # forward + backward + optimize\n",
        "    hidden_state = character_embedding(input_batch_torch)\n",
        "    loss = criterion(hidden_state, target_batch_torch)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Set the flag to evaluation, which will 'turn off' the dropout\n",
        "    character_embedding.eval()\n",
        "    \n",
        "    if epoch % display_interval == display_interval-1 : \n",
        "        print('Epoch: %d, loss: %.4f' %(epoch + 1, loss))\n",
        "\n",
        "print('Finished Training')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:50: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 10, loss: 0.3954\n",
            "Epoch: 20, loss: 0.4769\n",
            "Epoch: 30, loss: 0.4687\n",
            "Epoch: 40, loss: 0.4632\n",
            "Epoch: 50, loss: 0.4594\n",
            "Epoch: 60, loss: 0.4588\n",
            "Epoch: 70, loss: 0.4579\n",
            "Epoch: 80, loss: 0.4515\n",
            "Epoch: 90, loss: 0.4467\n",
            "Epoch: 100, loss: 0.4504\n",
            "Epoch: 110, loss: 0.4452\n",
            "Epoch: 120, loss: 0.4406\n",
            "Epoch: 130, loss: 0.4411\n",
            "Epoch: 140, loss: 0.4438\n",
            "Epoch: 150, loss: 0.4386\n",
            "Epoch: 160, loss: 0.4359\n",
            "Epoch: 170, loss: 0.4329\n",
            "Epoch: 180, loss: 0.4307\n",
            "Epoch: 190, loss: 0.4337\n",
            "Epoch: 200, loss: 0.4295\n",
            "Epoch: 210, loss: 0.4249\n",
            "Epoch: 220, loss: 0.4246\n",
            "Epoch: 230, loss: 0.4247\n",
            "Epoch: 240, loss: 0.4236\n",
            "Epoch: 250, loss: 0.4235\n",
            "Epoch: 260, loss: 0.4217\n",
            "Epoch: 270, loss: 0.4212\n",
            "Epoch: 280, loss: 0.4160\n",
            "Epoch: 290, loss: 0.4191\n",
            "Epoch: 300, loss: 0.4169\n",
            "Epoch: 310, loss: 0.4209\n",
            "Epoch: 320, loss: 0.4176\n",
            "Epoch: 330, loss: 0.4092\n",
            "Epoch: 340, loss: 0.4142\n",
            "Epoch: 350, loss: 0.4118\n",
            "Epoch: 360, loss: 0.4105\n",
            "Epoch: 370, loss: 0.4053\n",
            "Epoch: 380, loss: 0.4094\n",
            "Epoch: 390, loss: 0.4079\n",
            "Epoch: 400, loss: 0.4067\n",
            "Epoch: 410, loss: 0.4048\n",
            "Epoch: 420, loss: 0.4021\n",
            "Epoch: 430, loss: 0.4017\n",
            "Epoch: 440, loss: 0.4044\n",
            "Epoch: 450, loss: 0.4024\n",
            "Epoch: 460, loss: 0.4023\n",
            "Epoch: 470, loss: 0.3957\n",
            "Epoch: 480, loss: 0.3984\n",
            "Epoch: 490, loss: 0.3964\n",
            "Epoch: 500, loss: 0.3973\n",
            "Epoch: 510, loss: 0.3964\n",
            "Epoch: 520, loss: 0.3997\n",
            "Epoch: 530, loss: 0.3962\n",
            "Epoch: 540, loss: 0.3987\n",
            "Epoch: 550, loss: 0.3983\n",
            "Epoch: 560, loss: 0.4000\n",
            "Epoch: 570, loss: 0.3955\n",
            "Epoch: 580, loss: 0.3922\n",
            "Epoch: 590, loss: 0.3936\n",
            "Epoch: 600, loss: 0.3979\n",
            "Epoch: 610, loss: 0.3928\n",
            "Epoch: 620, loss: 0.3957\n",
            "Epoch: 630, loss: 0.3893\n",
            "Epoch: 640, loss: 0.3929\n",
            "Epoch: 650, loss: 0.3931\n",
            "Epoch: 660, loss: 0.3946\n",
            "Epoch: 670, loss: 0.3998\n",
            "Epoch: 680, loss: 0.3937\n",
            "Epoch: 690, loss: 0.3951\n",
            "Epoch: 700, loss: 0.3998\n",
            "Epoch: 710, loss: 0.3945\n",
            "Epoch: 720, loss: 0.4442\n",
            "Epoch: 730, loss: 0.4632\n",
            "Epoch: 740, loss: 0.4633\n",
            "Epoch: 750, loss: 0.4606\n",
            "Epoch: 760, loss: 0.4619\n",
            "Epoch: 770, loss: 0.4573\n",
            "Epoch: 780, loss: 0.4576\n",
            "Epoch: 790, loss: 0.4580\n",
            "Epoch: 800, loss: 0.4603\n",
            "Epoch: 810, loss: 0.4610\n",
            "Epoch: 820, loss: 0.4552\n",
            "Epoch: 830, loss: 0.4590\n",
            "Epoch: 840, loss: 0.4584\n",
            "Epoch: 850, loss: 0.4512\n",
            "Epoch: 860, loss: 0.4521\n",
            "Epoch: 870, loss: 0.4481\n",
            "Epoch: 880, loss: 0.4421\n",
            "Epoch: 890, loss: 0.4467\n",
            "Epoch: 900, loss: 0.4484\n",
            "Epoch: 910, loss: 0.4448\n",
            "Epoch: 920, loss: 0.4445\n",
            "Epoch: 930, loss: 0.4428\n",
            "Epoch: 940, loss: 0.4426\n",
            "Epoch: 950, loss: 0.4428\n",
            "Epoch: 960, loss: 0.4446\n",
            "Epoch: 970, loss: 0.4399\n",
            "Epoch: 980, loss: 0.4386\n",
            "Epoch: 990, loss: 0.4364\n",
            "Epoch: 1000, loss: 0.4347\n",
            "Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NrgPgntSxyXz",
        "colab_type": "text"
      },
      "source": [
        "* My character based model shows flacuation in loss sometimes. Based on what I investigated, this might be due to underperformed word model (skip-gram that I trained) or over-fitting. However, during the training, loss seems pretty meaningful."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "R5Bym9bBGlJE"
      },
      "source": [
        "### 2.2.4. Save Character Embeddings Model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ggTsYIm7GlJF",
        "outputId": "3fbcf495-6520-4580-b3dc-99bd2d583848",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "torch.save(character_embedding,'CharEmb_model_2.pt')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:360: UserWarning: Couldn't retrieve source code for container of type CharEmb. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "JwOI-wIKGlJI"
      },
      "source": [
        "### 2.2.5. Load Character Embeddings Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4-jyj-lOHWWj",
        "colab_type": "code",
        "outputId": "4dad73ac-d961-4253-dd2a-2a7dfdd2cda7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "downloaded = drive.CreateFile({'id':\"1IQnHB7wEMAmASqpvIpUn21uaQd4F1_HV\"})\n",
        "downloaded.GetContentFile('CharEmb_model_2.pt')\n",
        "\n",
        "char_model2 = torch.load('CharEmb_model_2.pt')\n",
        "char_model2.eval()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CharEmb(\n",
              "  (lstm): LSTM(36, 64, batch_first=True, dropout=0.2, bidirectional=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tlCeWT8eeLnd",
        "colab_type": "text"
      },
      "source": [
        "## 2.3. Sequence model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fwA-NN3EJ4Ig",
        "colab_type": "text"
      },
      "source": [
        "### 2.3.1. Apply/Import Word Embedding and Character Embedding Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UAMJrxx-iOVn",
        "colab_type": "text"
      },
      "source": [
        "* Since the embedding size from previous two models, word and character model, is  respectively 128, the sequence model's input size will be 128+128=256. Two models' embeddings are concatenated."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I3DV43omtVjd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "word_reviews_train = []\n",
        "word_reviews_train = list([item for sublist in reviews_train_fin for item in sublist])\n",
        "word_reviews_train = list(set(word_reviews_train))\n",
        "word_reviews_train.sort()\n",
        "\n",
        "# make a dictionary so that we can be reference each index of unique word\n",
        "word_dict_reviews_train = {w: i for i, w in enumerate(word_reviews_train)}\n",
        "\n",
        "word_length = len(max((word for L in reviews_train_fin for word in L), key=len))\n",
        "\n",
        "# match each word in the sentences with a number using a dictionary\n",
        "seq_input_data = []\n",
        "seq_input_data_all = []\n",
        "for seq in reviews_train_fin :\n",
        "  seq_input_data = [word_dict_reviews_train[n] for n in seq]\n",
        "  seq_input_data_all.append(seq_input_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DpYCL17JKZxl",
        "colab_type": "text"
      },
      "source": [
        "### 2.3.2. Build Sequence Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MpQvfCJHLuQQ"
      },
      "source": [
        "**Label Encoding**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FkcRYxaZLuQQ",
        "outputId": "d19bca00-5ac9-46cd-ad29-7d3a07477f4d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "import numpy as np\n",
        "\n",
        "# converting neg/pos label in the dataset into binary (0,1) to be classified\n",
        "labels = np.unique(sentiments_train)\n",
        "\n",
        "lEnc = LabelEncoder()\n",
        "lEnc.fit(labels)\n",
        "sentiments_train_n = lEnc.transform(sentiments_train)\n",
        "sentiments_test_n = lEnc.transform(sentiments_test)\n",
        "numClass = len(labels)\n",
        "\n",
        "print(labels)\n",
        "print(lEnc.transform(labels))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['neg' 'pos']\n",
            "[0 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g7PKX1gIePA2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_batch(input_embeddings, label, batch_size):\n",
        "    input_array = np.array(input_embedding)\n",
        "\n",
        "    # setting index randomly as a size of batch size \n",
        "    idx = np.random.randint(len(input_array), size=batch_size)\n",
        "\n",
        "    ha = input_embeddings[idx,:,:]\n",
        "    return ha, label[idx]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vvFrwZ6VOx9a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class SeqModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SeqModel, self).__init__()\n",
        "        self.lstm = nn.LSTM(n_input, n_hidden, batch_first = True)\n",
        "        self.linear = nn.Linear(n_hidden,n_class)\n",
        "\n",
        "    def forward(self, x):        \n",
        "        # lstm layer\n",
        "        x,_ = self.lstm(x)\n",
        "        # linear layer\n",
        "        x = self.linear(x[:,-1,:])\n",
        "        # softmax layer\n",
        "        x = F.log_softmax(x, dim=1)\n",
        "        # dimension should be 1 because we determine 0,1\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aCLUGzd6P3gY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import torch.optim as optim\n",
        "\n",
        "# Please assign values to these variables by using other variables (instead of hard code)\n",
        "seq_length = input_embeddings.shape[1]\n",
        "n_input = input_embeddings.shape[2] \n",
        "n_class = len(set(label))\n",
        "\n",
        "#Please decide the hyperparameters here by yourself\n",
        "n_hidden = 256\n",
        "batch_size = 300\n",
        "total_epoch = 500\n",
        "learning_rate = 0.3\n",
        "shown_interval = 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6BaOiaGRLW7R",
        "colab_type": "text"
      },
      "source": [
        "### 2.3.3. Train Sequence Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lVQnUSX1LZ6C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "net = Net().to(device)\n",
        "criterion = nn.NLLLoss()\n",
        "\n",
        "# Please find which optimizer provide higher f1\n",
        "optimizer = optim.Adam(net.parameters(), lr=learning_rate, )\n",
        "\n",
        "for epoch in range(total_epoch):\n",
        "\n",
        "    input_batch, target_batch = generate_batch(train_embeddings, train_label, batch_size)\n",
        "    input_batch_torch = torch.from_numpy(input_batch).float().to(device)\n",
        "    target_batch_torch = torch.from_numpy(target_batch).view(-1).to(device)\n",
        "\n",
        "    net.train()\n",
        "    outputs = net(input_batch_torch) \n",
        "    loss = criterion(outputs, target_batch_torch)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    if epoch % shown_interval == shown_interval-1:\n",
        "        net.eval()\n",
        "        outputs = net(input_batch_torch) \n",
        "        train_loss = criterion(outputs, target_batch_torch)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        train_acc= accuracy_score(predicted.cpu().numpy(),target_batch_torch.cpu().numpy())\n",
        "\n",
        "        print('Epoch: %d, train loss: %.5f, train_acc: %.4f'%(epoch + 1, train_loss.item(), train_acc))\n",
        "\n",
        "print('Finished Training')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-2feNpG-LZx2",
        "colab_type": "text"
      },
      "source": [
        "### 2.3.4. Save Sequence Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sflUAgV4L1o8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Please comment your code"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zFo6YppL6w3",
        "colab_type": "text"
      },
      "source": [
        "### 2.3.5. Load Sequence Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OtNxLzDGMCan",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Please comment your code"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4mpRpocePLN",
        "colab_type": "text"
      },
      "source": [
        "# 3 - Evaluation\n",
        "\n",
        "(*Please show your empirical evidence*)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KEW1zMgVMREr",
        "colab_type": "text"
      },
      "source": [
        "## 3.1. Performance Evaluation\n",
        "\n",
        "\n",
        "You are required to provide the table with precision, recall, f1 of test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LPHCb-bneTI9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "## Prediction\n",
        "net.eval()\n",
        "outputs = net(torch.from_numpy(test_embeddings).float().to(device)) \n",
        "_, predicted = torch.max(outputs, 1)\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(test_label, predicted.cpu().numpy(),digits=4))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P28Z1k36MZuo",
        "colab_type": "text"
      },
      "source": [
        "## 3.2. Hyperparameter Testing\n",
        "*You are required to draw a graph(y-axis: f1, x-axis: epoch) for test set and explain the optimal number of epochs based on the learning rate you have already chosen.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTLyQEeZMZ2f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Please comment your code"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}